import pandas as pd
from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter
import re
from typing import Any
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
from llama_index.core.query_engine import CustomQueryEngine
from llama_index.core.llms import LLM
from llama_index.llms.ollama import Ollama
from llama_index.llms.vertex import Vertex
from llama_index.core.llms import ChatMessage, MessageRole
import vertexai
from vertexai.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold
import asyncio
import nest_asyncio
nest_asyncio.apply()

from typing import Any, List, Callable, Optional, Union, Dict
from llama_index.core.async_utils import run_jobs
from llama_index.core.graph_stores.types import (
    EntityNode,
    KG_NODES_KEY,
    KG_RELATIONS_KEY,
    Relation,
)
from llama_index.core.prompts import PromptTemplate
from llama_index.core.schema import TransformComponent, BaseNode
from llama_index.core import Settings
from llama_index.core.graph_stores import SimplePropertyGraphStore
import networkx as nx
from graspologic.partition import hierarchical_leiden
from llama_index.core import PropertyGraphIndex
from llama_index.embeddings.ollama import OllamaEmbedding
from visualize_graphs import visualize_hierarchical_communities, visualize_original_graph, visualize_original_graph_detailed, visualize_node_relationship, visualize_entity_network, create_community_wordclouds,visualize_community_relationships
import time


GCP_PROJECT = "finrisk-sandbox"
GCP_LOCATION = "us-central1"
vertexai.init(project= GCP_PROJECT, location="us-central1")
# Configure safety settings to be more permissive
safety_settings = {
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}

llm = Vertex(model="gemini-2.5-flash-lite", safety_settings=safety_settings, max_tokens=15000)
embeddings = OllamaEmbedding(
    model_name="tinyLlama:latest",  # Match the LLM model
)


# Simplified prompt that's easier for the LLM to follow
KG_TRIPLET_EXTRACT_TMPL = """
-Goal-
Given a text document, identify all important entities and their types, and extract all relationships among the identified entities.
Extract up to {max_knowledge_triplets} of the most important entity-relation triplets.

-Steps-
1. Identify all important ENTITIES from the text. For each entity, extract:
   - Entity name: Name of the entity (capitalized, use full names when possible)
   - Entity type: One of: person, organization, location, event, concept, object
   - Entity description: Comprehensive description of the entity's attributes, role, and activities

   Format each entity as:
   ENTITY: [entity name]
   TYPE: [entity type]
   DESC: [entity description]

2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly and directly related* to each other.
   Only extract relationships where there is an explicit connection mentioned in the text.

   For each pair of related entities, extract:
   - Source entity: name of the source entity (exactly as identified in step 1)
   - Target entity: name of the target entity (exactly as identified in step 1)
   - Relationship type: Specific relationship type (e.g., works_for, located_in, part_of, involved_in, created_by, member_of, related_to, etc.)
   - Relationship description: Clear explanation of why these entities are related, based on the text

   Format each relationship as:
   RELATIONSHIP: [source entity] -> [target entity]
   TYPE: [relationship type]
   DESC: [relationship description]

3. Prioritize the most important and clearly stated entities and relationships. Do not extract vague or inferred connections.

-Examples-

Example 1 (Correct Entity Naming):
Text mentions: "Bacon studied at Trinity College" and "Francis Bacon was a philosopher"
Extract as:
ENTITY: Francis Bacon
TYPE: person
DESC: English philosopher and statesman

RELATIONSHIP: Francis Bacon -> Trinity College
TYPE: attended
DESC: Francis Bacon studied at Trinity College Cambridge

Example 2:
ENTITY: Trinity College
TYPE: organization
DESC: University college in Cambridge where Francis Bacon studied

RELATIONSHIP: Trinity College -> Cambridge
TYPE: located_in
DESC: Trinity College is located in the city of Cambridge

-Important Notes-
- Use the EXACT format shown above with ENTITY:, TYPE:, DESC:, RELATIONSHIP:, and -> symbols
- Entity names must match exactly between entities and relationships
- **Maintain consistent naming: if an entity is referred to by different names (e.g., "Francis Bacon" and "Bacon"), use the full canonical name consistently**
- Only extract relationships that are explicitly stated or clearly implied in the text
- **Do NOT extract trivial, obvious, or common-sense relationships (e.g., "hand fits into glove", "person wears clothes", "book has pages")**
- Focus on quality over quantity - extract the most important connections
- If an entity appears multiple times, use the same name consistently
- Bacon is the same person as Francis Bacon and all entities of Bacon must be classified as Francis Bacon. 

-Real Data-
TEXT: {text}

Now analyze this text and extract entities and relationships following the format above:
"""


# Load markdown file
with open("src2/novel.md", "r", encoding="utf-8") as f:
    novel_text = f.read()


# Create a single document from the entire text
documents = [Document(text=novel_text)]

# Use SentenceSplitter to chunk the document
splitter = SentenceSplitter(
    chunk_size=1024,  # Reduced chunk size for better extraction
    chunk_overlap=20,
)

nodes = splitter.get_nodes_from_documents(documents)

# Simplified patterns that are easier for the LLM to follow
entity_pattern = r'ENTITY:\s*(.+?)\s*TYPE:\s*(.+?)\s*DESC:\s*(.+?)(?=\s*ENTITY:|\s*RELATIONSHIP:|\s*$)'
relationship_pattern = r'RELATIONSHIP:\s*(.+?)\s*->\s*(.+?)\s*TYPE:\s*(.+?)\s*DESC:\s*(.+?)(?=\s*ENTITY:|\s*RELATIONSHIP:|\s*$)'

def canonicalize_bacon(entity_name: str) -> str:
    """
    Force all Bacon variants to canonical 'Francis Bacon'.
    Set a breakpoint whenever a canonicalization happens.
    """
    name_clean = entity_name.strip()

    if name_clean == 'Bacon' or name_clean == 'Sir Francis Bacon':
        # Trigger breakpoint only when conversion occurs
        print(f"[CANONICALIZING] '{entity_name}' -> 'Francis Bacon'")
        a=1
        return "Francis Bacon"
    
    return name_clean

def parse_fn(response_str: str) -> Any:
    """More robust parsing function"""
    print(f"Raw LLM response: {response_str}")  # Debug: see what the LLM is producing
    
    entities = re.findall(entity_pattern, response_str, re.DOTALL)
    relationships = re.findall(relationship_pattern, response_str, re.DOTALL)

    canonical_entities = []
    for name, ent_type, desc in entities:
        canonical_name = canonicalize_bacon(name)
        canonical_entities.append((canonical_name, ent_type, desc))

    # --- Canonicalize references inside relationships
    canonical_relationships = []
    for src, tgt, rel_type, desc in relationships:
        canonical_src = canonicalize_bacon(src)
        canonical_tgt = canonicalize_bacon(tgt)
        canonical_relationships.append((canonical_src, canonical_tgt, rel_type, desc))
    
    print(f"Extracted entities: {entities}")  # Debug
    print(f"Extracted relationships: {relationships}")  # Debug
    
    return canonical_entities, canonical_relationships

class GraphRAGExtractor(TransformComponent):
    """Fixed GraphRAG extractor with better error handling"""

    llm: LLM
    extract_prompt: PromptTemplate
    parse_fn: Callable
    num_workers: int
    max_paths_per_chunk: int

    def __init__(
        self,
        llm: Optional[LLM] = None,
        extract_prompt: Optional[Union[str, PromptTemplate]] = None,
        parse_fn: Callable = parse_fn,
        max_paths_per_chunk: int = 5,  # Increased for better coverage
        num_workers: int = 2,  # Reduced for stability
    ) -> None:


        if isinstance(extract_prompt, str):
            extract_prompt = PromptTemplate(extract_prompt)

        super().__init__(
            llm=llm or Settings.llm,
            extract_prompt=extract_prompt,
            parse_fn=parse_fn,
            num_workers=num_workers,
            max_paths_per_chunk=max_paths_per_chunk,
        )

    @classmethod
    def class_name(cls) -> str:
        return "GraphExtractor"

    def __call__(
        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any
    ) -> List[BaseNode]:
        return asyncio.run(
            self.acall(nodes, show_progress=show_progress, **kwargs)
        )

    async def _aextract(self, node: BaseNode) -> BaseNode:
        """Extract triples from a node with better error handling"""
        assert hasattr(node, "text")

        text = node.get_content(metadata_mode="llm")
        print(f"Processing text: {text[:100]}...")  # Debug
        
        try:
            llm_response = await self.llm.apredict(
                self.extract_prompt,
                text=text,
                max_knowledge_triplets=self.max_paths_per_chunk,
            )
            print(f"LLM Response: {llm_response}")  # Debug
            
            entities, entities_relationship = self.parse_fn(llm_response)
            print(f"Parsed - Entities: {len(entities)}, Relationships: {len(entities_relationship)}")  # Debug
            
        except Exception as e:
            print(f"Error during extraction: {e}")
            entities = []
            entities_relationship = []

        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])
        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])
        
        # Add entities
        for entity_name, entity_type, description in entities:
            entity_node = EntityNode(
                name=entity_name.strip(),
                label=entity_type.strip(),
                properties={"description": description.strip()}
            )
            existing_nodes.append(entity_node)

        # Add relationships
        for source, target, relation, description in entities_relationship:
            source_node = EntityNode(name=source.strip(), properties={})
            target_node = EntityNode(name=target.strip(), properties={})
            
            rel_node = Relation(
                label=relation.strip(),
                source_id=source_node.id,
                target_id=target_node.id,
                properties={"description": description.strip()},
            )

            existing_nodes.extend([source_node, target_node])
            existing_relations.append(rel_node)

        node.metadata[KG_NODES_KEY] = existing_nodes
        node.metadata[KG_RELATIONS_KEY] = existing_relations
        
        print(f"Final - Nodes: {len(existing_nodes)}, Relations: {len(existing_relations)}")  # Debug
        return node

    async def acall(
        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any
    ) -> List[BaseNode]:
        jobs = []
        for node in nodes:
            jobs.append(self._aextract(node))

        return await run_jobs(
            jobs,
            workers=self.num_workers,
            show_progress=show_progress,
            desc="Extracting paths from text",
        )

kg_extractor = GraphRAGExtractor(
    llm=llm,
    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,
    max_paths_per_chunk=3,
    parse_fn=parse_fn,
    num_workers=1,  # Start with 1 worker for debugging
)

class GraphRAGStore(SimplePropertyGraphStore):
    community_summary = {}
    max_cluster_size = 20

    def generate_community_summary(self, text):
        """Generate summary for a given text using an LLM."""
        messages = [
            ChatMessage(
                role="system",
                content=(
                    "You are provided with a set of relationships from a knowledge graph, each represented as "
                    "entity1->entity2->relation->relationship_description. Your task is to create a summary of these "
                    "relationships. The summary should include the names of the entities involved and a concise synthesis "
                    "of the relationship descriptions. The goal is to capture the most critical and relevant details that "
                    "highlight the nature and significance of each relationship. Ensure that the summary is coherent and "
                    "integrates the information in a way that emphasizes the key aspects of the relationships."
                ),
            ),
            ChatMessage(role="user", content=text),
        ]
        llm = Vertex(model="gemini-2.5-flash-lite", safety_settings=safety_settings, max_tokens=10000)
        response = llm.chat(messages=messages)
        # response = Ollama(model="llama3.2:3b", request_timeout=6000.0).chat(messages)
        clean_response = re.sub(r"^assistant:\s*", "", str(response)).strip()
        return clean_response
    
    def get_community_graph(self):
        """Creates a simplified graph where nodes are communities."""
        nx_graph = self._create_nx_graph()
        community_hierarchical_clusters = hierarchical_leiden(
            nx_graph, max_cluster_size=self.max_cluster_size
        )
        
        # Create mapping: node -> community_id
        community_mapping = {item.node: item.cluster for item in community_hierarchical_clusters}
        
        # Create simplified community graph
        community_graph = nx.Graph()
        
        # Add community nodes with summaries
        community_nodes = {}
        for cluster_id in set(community_mapping.values()):
            summary = self.community_summary.get(cluster_id, f"Community {cluster_id}")
            # Truncate summary for display
            short_summary = summary[:100] + "..." if len(summary) > 100 else summary
            community_nodes[cluster_id] = short_summary
            community_graph.add_node(cluster_id, summary=summary, label=f"Community {cluster_id}")
        
        # Add edges between communities (if nodes in different communities are connected)
        inter_community_edges = {}
        for u, v, data in nx_graph.edges(data=True):
            comm_u = community_mapping.get(u)
            comm_v = community_mapping.get(v)
            
            if comm_u != comm_v:  # Edge between different communities
                if (comm_u, comm_v) not in inter_community_edges:
                    inter_community_edges[(comm_u, comm_v)] = []
                inter_community_edges[(comm_u, comm_v)].append(data.get('relationship', 'unknown'))
        
        # Add edges to community graph (weight = number of connections)
        for (comm_u, comm_v), relationships in inter_community_edges.items():
            community_graph.add_edge(comm_u, comm_v, 
                                    weight=len(relationships),
                                    relationships=relationships)
        
        return community_graph, community_nodes
    
    def get_communities_at_level(self, level=0):
        """
        Extract communities at a specific hierarchical level.
        Level 0 = root/coarse communities (larger)
        Level 1 = sub-communities (finer)
        Higher levels = even finer granularity
        """
        nx_graph = self._create_nx_graph()
        
        # For level 0, use a larger max_cluster_size to get coarser communities
        # For level 1, use smaller max_cluster_size to get finer communities
        if level == 0:
            # Root level: larger communities (less granular)
            max_size = max(50, len(nx_graph.nodes()) // 5)  # Adaptive based on graph size
        elif level == 1:
            # Level 1: finer communities (more granular)
            max_size = self.max_cluster_size  # Use your existing setting
        else:
            # Even finer levels
            max_size = max(2, self.max_cluster_size // (level + 1))
        
        community_hierarchical_clusters = hierarchical_leiden(
            nx_graph, max_cluster_size=max_size
        )
        
        # Create mapping: node -> community_id at this level
        community_mapping = {item.node: item.cluster for item in community_hierarchical_clusters}
        
        return community_mapping, nx_graph
    
    def build_communities(self):
        """Builds communities from the graph and summarizes them."""
        nx_graph = self._create_nx_graph()
        community_hierarchical_clusters = hierarchical_leiden(
            nx_graph, max_cluster_size=self.max_cluster_size
        )
        community_info = self._collect_community_info(
            nx_graph, community_hierarchical_clusters
        )
        self._summarize_communities(community_info)

    def _create_nx_graph(self):
        """Converts internal graph representation to NetworkX graph."""
        nx_graph = nx.Graph()
        for node in self.graph.nodes.values():
            if node.id == "Bacon":
              a=1
            nx_graph.add_node(str(node))
        for relation in self.graph.relations.values():
            if relation.source_id == 'Bacon':
                a=1
            if relation.target_id == 'Bacon':
                a=1

            nx_graph.add_edge(
                relation.source_id,
                relation.target_id,
                relationship=relation.label,
                description=relation.properties["description"],
            )
        return nx_graph

    def _collect_community_info(self, nx_graph, clusters):
        """Collect detailed information for each node based on their community."""
        community_mapping = {item.node: item.cluster for item in clusters}
        community_info = {}
        for item in clusters:
            cluster_id = item.cluster
            node = item.node
            if cluster_id not in community_info:
                community_info[cluster_id] = []

            for neighbor in nx_graph.neighbors(node):
                if community_mapping[neighbor] == cluster_id:
                    edge_data = nx_graph.get_edge_data(node, neighbor)
                    if edge_data:
                        detail = f"{node} -> {neighbor} -> {edge_data['relationship']} -> {edge_data['description']}"
                        community_info[cluster_id].append(detail)
        return community_info

    def _summarize_communities(self, community_info):
        """Generate and store summaries for each community."""
        for community_id, details in community_info.items():
            if len(details) > 0:
             details_text = (
                "\n".join(details) + ".")  # Ensure it ends with a period
                
             self.community_summary[community_id] = self.generate_community_summary(details_text)
            else:
               self.community_summary[community_id] = 'None' 

    def get_community_summaries(self):
        """Returns the community summaries, building them if not already done."""
        with open("community_summaries.pkl", "rb") as f:
          import pickle
          self.community_summary = pickle.load(f)
        if not self.community_summary:
            self.build_communities()
        return self.community_summary




# Create index with debug info
print("Creating PropertyGraphIndex...")
# index = PropertyGraphIndex(
#     nodes=nodes,  # Start with just 10 nodes for testing
#     llm=llm,
#     embed_model=embeddings,
#     property_graph_store=GraphRAGStore(),
#     kg_extractors=[kg_extractor],
#     show_progress=True,
# )

PERSIST_DIR = "Novel2"
from llama_index.core import (
    PropertyGraphIndex,
    StorageContext,
    load_index_from_storage,
)

# Store the index so that it can be loaded in later if necessary. Uncomment the code below to load in the index. 

graph_store = GraphRAGStore.from_persist_dir(PERSIST_DIR)

storage_context = StorageContext.from_defaults(
    persist_dir=PERSIST_DIR,
    property_graph_store=graph_store
)

index = load_index_from_storage(storage_context, llm=llm, embed_model=embeddings)

# Check if we have any data in the graph
print(f"Graph store nodes: {len(index.property_graph_store.graph.nodes)}")
print(f"Graph store relations: {len(index.property_graph_store.graph.relations)}")
# create_community_wordclouds(index, [128, 210])

if (index.property_graph_store.graph.nodes and 
    index.property_graph_store.graph.relations):
    #index.storage_context.persist(persist_dir="Novel2")
    print("Building communities...")
    #index.property_graph_store.build_communities()
    #index.storage_context.persist(persist_dir=PERSIST_DIR)
   
else:
    print("No graph data extracted. Check the LLM extraction output.")

#index.storage_context.persist(persist_dir=PERSIST_DIR)

# Visualize the simplified community graph

class GraphRAGQueryEngine(CustomQueryEngine):
    graph_store: GraphRAGStore
    llm: LLM

    def custom_query(self, query_str: str) -> Dict:
        """
        The synchronous wrapper that the user calls.
        It sets up and runs the main asynchronous logic.
        """
        print("--- Kicking off parallel query execution ---")
        start_time = time.time()

        # Use asyncio.run() to execute the async version of the query method
        results = asyncio.run(self.a_custom_query(query_str))

        end_time = time.time()
        total_time = end_time - start_time
        print(f"\n--- Parallel execution finished in {total_time:.2f} seconds ---")
        
        # Add the total time to the final results for proof
        results['total_processing_time'] = total_time
        return results
    
    async def a_custom_query(self, query_str: str) -> Dict:
        """
        The core asynchronous logic that runs all LLM calls concurrently.
        """
        community_summaries = self.graph_store.get_community_summaries()
        
        # --- Create a list of tasks to run in parallel ---
        # Instead of a for loop that waits, we create a list of "coroutines"
        # that can all be started at once.
        tasks = []
        for community_id, summary in community_summaries.items():
            task = self.a_generate_answer_from_summary(summary, query_str, community_id)
            tasks.append(task)
        
        # --- Run all tasks concurrently and wait for them all to finish ---
        # asyncio.gather() is the magic that runs everything at the same time.
        community_results = await asyncio.gather(*tasks)
        
        # --- Process the results after they are all complete ---
        contributing_communities = [res for res in community_results if res['contributed']]
        
        if not contributing_communities:
            return {
                'answer': "No relevant information found to answer your question.",
                'contributing_communities': [],
                'all_communities': community_results,
            }

        # Aggregate the final answer from the communities that contributed
        final_answer = await self.a_aggregate_answers([r['answer'] for r in contributing_communities])

        return {
            'answer': final_answer,
            'contributing_communities': contributing_communities,
            'all_communities': community_results,
            'num_contributing': len(contributing_communities),
            'total_communities': len(community_results)
        }
    
    async def a_generate_answer_from_summary(self, community_summary, query, community_id):
        """
        The ASYNCHRONOUS version of the LLM call function.
        """
        # Your original logic is mostly the same
        if community_summary == 'None': # Your check
            answer = 'null'
        else:
            prompt = (
                f"QUERY: {query}\n\n"
                f"COMMUNITY SUMMARY: {community_summary}\n\n"
                "INSTRUCTIONS:\n"
                "1. If the community summary is relevant, provide a concise answer.\n"
                "2. If not relevant, output exactly: null\n\n"
                "OUTPUT:"
            )
            messages = [ChatMessage(role="user", content=prompt)]
            
            # --- Use the async `achat` method for the LLM call ---
            response = await self.llm.achat(messages)
            answer = re.sub(r"^assistant:\s*", "", str(response)).strip()

        is_contributed = answer.lower() != "null" and answer.strip() != "" and answer != 'assistant: null'
        if is_contributed:
            a=1
        return {
            'community_id': community_id,
            'summary': community_summary,
            'answer': answer,
            'contributed': is_contributed
        }

    def aggregate_answers(self, community_answers):
        """Aggregate individual community answers into a final, coherent response."""
        # intermediate_text = " ".join(community_answers)
        prompt = "Combine the following intermediate answers into a final, concise response."
        messages = [
            ChatMessage(role="system", content=prompt),
            ChatMessage(
                role="user",
                content=f"Intermediate answers: {community_answers}",
            ),
        ]
        final_response = self.llm.chat(messages)
        cleaned_final_response = re.sub(
            r"^assistant:\s*", "", str(final_response)
        ).strip()
        return cleaned_final_response
    
    async def a_aggregate_answers(self, relevant_answers: List[str]) -> str:
        """Asynchronous aggregation of final answers."""
        prompt = "Combine the following pieces of information into a final, coherent answer."
        answers_str = "\n".join([f"- {ans}" for ans in relevant_answers])
        messages = [
            ChatMessage(role="system", content=prompt),
            ChatMessage(role="user", content=f"Information found:\n{answers_str}"),
        ]
        response = await self.llm.achat(messages)
        cleaned_final_response = re.sub(
            r"^assistant:\s*", "", str(response)
        ).strip()
        return cleaned_final_response

llm = Vertex(model="gemini-2.5-flash-lite", safety_settings=safety_settings, max_tokens=1000)

query_engine = GraphRAGQueryEngine(
    graph_store=index.property_graph_store, llm=llm
)

response = query_engine.query("Who claimed that knowledge causes anxiety, discontent and rebellion?")
print(f"{response['answer']}")

contributing_communities = response['contributing_communities']

community_ids = []
for comm in contributing_communities:
 community_ids.append(comm['community_id'])

# Usage:
visualize_community_relationships(index, community_ids)
create_community_wordclouds(index, community_ids)
visualize_hierarchical_communities(index)
visualize_original_graph(index)
visualize_original_graph_detailed(index)
